import os 
import yaml
import json
import requests
from termcolor import colored
from utilities.prompts import planning_agent_prompt, integration_agent_prompt
from tools.search import WebSearcher


def load_config(file_path):
    """
    Load configuration from a YAML file and set environment variables.

    Args:
        file_path (str): The path to the YAML file containing the configuration.

    Returns:
        None

    This function reads a YAML file and sets environment variables based on the key-value pairs in the file.
    It uses the `yaml.safe_load()` function to parse the YAML file and then iterates over each key-value pair
    in the resulting dictionary. For each pair, it sets the corresponding environment variable using the `os.environ`
    dictionary.

    Note:
        This function assumes that the YAML file contains key-value pairs that can be directly used as environment
        variables. It does not perform any validation or sanitization of the values.

    Example:
        Suppose you have a YAML file named `config.yaml` with the following content:

        ```yaml
        API_KEY: abc123
        DB_HOST: localhost
        DB_PORT: 5432
        ```
    
        You can load this configuration file and set the environment variables using the following code:

        ```python
        load_config('config.yaml')
        ```

        After calling `load_config()`, the environment variables `API_KEY`, `DB_HOST`, and `DB_PORT` will be set to
    """
    with open(file_path, 'r') as file:
        config = yaml.safe_load(file)
        for key, value in config.items():
            os.environ[key] = value

class Agent:
    def __init__(self, model, tool, temperature=0, max_tokens=1000, planning_agent_prompt=None, integration_agent_prompt=None, verbose=False):
        """
        Initializes an instance of the Agent class.

        Args:
            model (str): The name of the OpenAI model to use.
            tool (object): An instance of the tool to be used.
            temperature (float, optional): The temperature parameter for the OpenAI model. Defaults to 0.
            max_tokens (int, optional): The maximum number of tokens to generate. Defaults to 1000.
            planning_agent_prompt (str, optional): The prompt for the planning agent. Defaults to None.
            integration_agent_prompt (str, optional): The prompt for the integration agent. Defaults to None.
            verbose (bool, optional): Whether to print verbose output. Defaults to False.

        Initializes the following instance variables:
            - api_key (str): The OpenAI API key.
            - url (str): The URL for the OpenAI API.
            - headers (dict): The headers for the API request.
            - temperature (float): The temperature parameter.
            - max_tokens (int): The maximum number of tokens.
            - tool (object): The tool instance.
            - tool_specs (str): The documentation of the tool.
            - planning_agent_prompt (str): The prompt for the planning agent.
            - integration_agent_prompt (str): The prompt for the integration agent.
            - model (str): The name of the OpenAI model.
            - verbose (bool): Whether to print verbose output.
        """
        load_config('config.yaml')
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.url = 'https://api.openai.com/v1/chat/completions'
        self.headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {self.api_key}'
        }
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.tool = tool
        self.tool_specs = tool.__doc__
        self.planning_agent_prompt = planning_agent_prompt
        self.integration_agent_prompt = integration_agent_prompt
        self.model = model
        self.verbose = verbose
    
    def run_planning_agent(self, query, plan=None, outputs=None, feedback=None):
        """
        Runs the planning agent to generate a plan for a given query.

        Args:
            query (str): The query to generate a plan for.
            plan (str, optional): The previous plan generated by the planning agent. Defaults to None.
            outputs (dict, optional): The outputs generated by the integration agent. Defaults to None.
            feedback (str, optional): The feedback provided by the user. Defaults to None.

        Returns:
            str: The generated plan from the planning agent.

        This function formats the system prompt for the planning agent using the provided `outputs`, `plan`, `feedback`, and `tool_specs`. 
        It then creates a dictionary containing the necessary data for the API request, including the model, messages, temperature, and max_tokens. 
        The dictionary is converted to JSON and sent as the request body to the OpenAI API. 
        The response is parsed to extract the generated plan from the API response and printed in green color. 
        The generated plan is then returned.
        """

        system_prompt = self.planning_agent_prompt.format(
            outputs=outputs,
            plan=plan,
            feedback=feedback,
            tool_specs=self.tool_specs
        )

        data = {
            "model": self.model,
            "messages": [{"role": "user", "content": query},
                         {"role": "system", "content": system_prompt}],
            "temperature": self.temperature,
            "max_tokens": self.max_tokens
        }

        json_data = json.dumps(data)
        response = requests.post(self.url, headers=self.headers, data=json_data, timeout=180)
        response_dict = response.json()
        content = response_dict['choices'][0]['message']['content']
        print(colored(f"Planning Agent: {content}", 'green'))

        return content
    
    def run_integration_agent(self, query, plan, outputs):
        """
        Runs the integration agent to generate a response based on a given query, plan, and outputs.

        Args:
            query (str): The query to generate a response for.
            plan (str): The plan generated by the planning agent.
            outputs (dict): The outputs generated by the integration agent.

        Returns:
            str: The generated response from the integration agent.

        This function formats the system prompt for the integration agent using the provided `outputs` and `plan`. 
        It then creates a dictionary containing the necessary data for the API request, including the model, messages, 
        temperature, and max_tokens. The dictionary is converted to JSON and sent as the request body to the OpenAI API. 
        The response is parsed to extract the generated response from the API response and printed in blue color. 
        The generated response is then returned.
        """
        system_prompt = self.integration_agent_prompt.format(
            outputs=outputs,
            plan=plan
        )

        data = {
            "model": self.model,
            "messages": [{"role": "user", "content": query},
                         {"role": "system", "content": system_prompt}],
            "temperature": self.temperature,
            "max_tokens": self.max_tokens
        }

        json_data = json.dumps(data)
        response = requests.post(self.url, headers=self.headers, data=json_data, timeout=180)
        response_dict = response.json()
        content = response_dict['choices'][0]['message']['content']
        print(colored(f"Integration Agent: {content}", 'blue'))
        # print("Integration Agent:", content)

        return content
    
    def check_response(self, response, query):
        """
        Check if the response meets the requirements of the query based on the following:
        1. The response should be relevant to the query.
        2. The response should be coherent and well-structured with citations.
        3. The response should be comprehensive and address the query in its entirety.
        Return 'True' if the response meets the requirements and 'False' otherwise.

        Args:
            response (str): The response to check.
            query (str): The query to check against.

        Returns:
            bool: True if the response meets the requirements, False otherwise.
        """
        
        tools = [
            {
                "type": "function",
                "function": {
                    "name": "respose_checker",
                    "description": "Checck if the response meets the requirements",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "meets_requirements": {
                                "type": "string",
                                "description": """Check if the response meets the requirements of the query based on the following:
                                1. The response should be relevant to the query.
                                2. The response should be coherent and well-structured with citations.
                                3. The response should be comprehensive and address the query in its entirety.
                                Return 'yes' if the response meets the requirements and 'no' otherwise.
                                """
                            },
                        },
                        "required": ["meets_requirements"]
                    }
                }
            }
        ]

        data = {
            "model": self.model,
            "messages": [{"role": "user", "content": f"Response: {response} \n Query: {query}"},],
            "temperature": 0,
            "tools": tools,
            "tool_choice": "required"
        }

        json_data = json.dumps(data)
        response = requests.post(self.url, headers=self.headers, data=json_data, timeout=180)
        response_dict = response.json()

        tool_calls = response_dict['choices'][0]['message']['tool_calls'][0]
        arguments_json = json.loads(tool_calls['function']['arguments'])
        response = arguments_json['meets_requirements']

        if response == 'yes':
            return True
        else:
            return False

         
    def execute(self, iterations=5):
        """
        Executes the query processing pipeline for a given number of iterations.

        Args:
            iterations (int, optional): The number of iterations to run the pipeline. Defaults to 5.

        Returns:
            None

        This function prompts the user to enter a query and initializes the necessary variables. 
        It then enters a loop that runs until the requirements are met or the maximum number of iterations is reached.
        Within each iteration, the function performs the following steps:
        1. Calls the `run_planning_agent` method to generate a plan based on the query, previous plan, outputs, and feedback.
        2. Calls the `use_tool` method of the `tool` object to generate outputs based on the plan and query.
        3. Calls the `run_integration_agent` method to get a response based on the query, plan, and outputs.
        4. Calls the `check_response` method to determine if the response meets the requirements.

        After the loop, the function prints the final response in cyan color.
        """
        query = input("Enter your query: ")
        tool =  self.tool(model=self.model, verbose=self.verbose)
        meets_requirements = False
        plan = None
        outputs = None
        response = None
        iterations = 0

        while not meets_requirements and iterations < 5:
            iterations += 1  
            plan = self.run_planning_agent(query, plan=plan, outputs=outputs, feedback=response)
            outputs = tool.use_tool(plan=plan, query=query)
            response = self.run_integration_agent(query, plan, outputs)
            meets_requirements = self.check_response(response, query)

        print(colored(f"Final Response: {response}", 'cyan'))

        
if __name__ == '__main__':
    agent = Agent(model="gpt-3.5-turbo",
                  tool=WebSearcher, 
                  planning_agent_prompt=planning_agent_prompt, 
                  integration_agent_prompt=integration_agent_prompt,
                  verbose=True
                  )
    agent.execute()